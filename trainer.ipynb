{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch of images shape: torch.Size([64, 1, 100, 100])\n",
      "Training batch of labels shape: torch.Size([64])\n",
      "Test batch of images shape: torch.Size([64, 1, 100, 100])\n",
      "Test batch of labels shape: torch.Size([64])\n",
      "train_size: 54000, test_size: 6000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "57600"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "\n",
    "class ClutteredMNISTDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Custom dataset for Cluttered MNIST.\n",
    "        :param root_dir: Root directory of the dataset (e.g., \"dataset/cluttered_mnist\")\n",
    "        :param transform: Optional torchvision transforms to apply to the images\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Gather all image paths and their labels\n",
    "        self.data = []\n",
    "        for label in range(10):  # Assuming labels are 0-9\n",
    "            label_dir = os.path.join(root_dir, str(label))\n",
    "            if os.path.isdir(label_dir):\n",
    "                for file_name in os.listdir(label_dir):\n",
    "                    if file_name.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                        file_path = os.path.join(label_dir, file_name)\n",
    "                        self.data.append((file_path, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve an image and its label at the specified index.\n",
    "        :param idx: Index of the data point\n",
    "        :return: Tuple (image, label)\n",
    "        \"\"\"\n",
    "        image_path, label = self.data[idx]\n",
    "        image = Image.open(image_path).convert('L')  # Convert to grayscale\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Define transforms for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((100, 100)),  # Resize images to 100x100\n",
    "    transforms.ToTensor(),          # Convert images to PyTorch tensors\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # Normalize using MNIST mean and std\n",
    "])\n",
    "\n",
    "# Create dataset\n",
    "dataset_dir = \"dataset/cluttered_mnist\"\n",
    "cluttered_mnist_dataset = ClutteredMNISTDataset(root_dir=dataset_dir, transform=transform)\n",
    "\n",
    "# Split dataset into train and test (80% train, 20% test)\n",
    "train_size = int(0.9 * len(cluttered_mnist_dataset))\n",
    "test_size = len(cluttered_mnist_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(cluttered_mnist_dataset, [train_size, test_size])\n",
    "\n",
    "# DataLoader for batching and shuffling\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Example: Iterate through the DataLoader\n",
    "for images, labels in train_loader:\n",
    "    print(\"Training batch of images shape:\", images.shape)  # (batch_size, 1, 100, 100)\n",
    "    print(\"Training batch of labels shape:\", labels.shape)  # (batch_size,)\n",
    "    break\n",
    "\n",
    "for images, labels in test_loader:\n",
    "    print(\"Test batch of images shape:\", images.shape)  # (batch_size, 1, 100, 100)\n",
    "    print(\"Test batch of labels shape:\", labels.shape)  # (batch_size,)\n",
    "    break\n",
    "\n",
    "print(f\"train_size: {train_size}, test_size: {test_size}\")\n",
    "64*900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from glimpse import GlimpseModel\n",
    "\n",
    "\"\"\"print('the code god was here')\"\"\"\n",
    "# Define the RNN model\n",
    "class MNISTRNN(nn.Module):\n",
    "    def __init__(self, crop_size, hidden_size, num_layers, num_classes):\n",
    "        super(MNISTRNN, self).__init__()\n",
    "        self.crop_size = crop_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # RNN to process the crops\n",
    "        self.rnn = nn.RNN(crop_size**2, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc_class = nn.Linear(hidden_size, num_classes)  # Class prediction\n",
    "        self.fc_action = nn.Linear(hidden_size, 2)  # Next crop center prediction\n",
    "\n",
    "    def crop(self, padded_image, center):\n",
    "        \"\"\"\n",
    "        Crop a region around the given center from the padded image.\n",
    "        :param padded_image: Padded input image (batch_size, 1, padded_size, padded_size)\n",
    "        :param center: Crop centers (batch_size, 2)\n",
    "        :return: Cropped image regions (batch_size, crop_size, crop_size)\n",
    "        \"\"\"\n",
    "        crop_size = self.crop_size\n",
    "        half_crop = crop_size // 2\n",
    "\n",
    "        # Compute cropping indices\n",
    "        x_start = (center[:, 0] - half_crop).long()\n",
    "        x_end = (center[:, 0] + half_crop).long()\n",
    "        y_start = (center[:, 1] - half_crop).long()\n",
    "        y_end = (center[:, 1] + half_crop).long()\n",
    "\n",
    "        # Perform efficient tensor slicing\n",
    "        crops = torch.stack([\n",
    "            padded_image[b, :, y_start[b].item():y_end[b].item(), x_start[b].item():x_end[b].item()]\n",
    "            for b in range(padded_image.size(0))\n",
    "        ])\n",
    "        return crops\n",
    "\n",
    "    def forward(self, image, center, h0):\n",
    "        \"\"\"\n",
    "        Forward pass with dynamic cropping and RNN processing.\n",
    "        :param image: Full input image (batch_size, 1, 28, 28)\n",
    "        :param center: Initial crop centers (batch_size, 2)\n",
    "        :param h0: Initial hidden state (num_layers, batch_size, hidden_size)\n",
    "        :return: Class prediction, next center, hidden state\n",
    "        \"\"\"\n",
    "        # batch_size = image.size(0)\n",
    "        # crop_size = self.crop_size\n",
    "\n",
    "        # Pad the image to avoid boundary issues\n",
    "        # padding = crop_size // 2\n",
    "        # padded_image = nn.functional.pad(image, (padding, padding, padding, padding), mode='constant', value=0)\n",
    "\n",
    "        # Extract crops\n",
    "        # crops = self.crop(padded_image, center)  # (batch_size, 1, crop_size, crop_size)\n",
    "        # crops = crops.view(batch_size, -1)  # Flatten the crop (batch_size, crop_size^2)\n",
    "\n",
    "        # # Process with RNN\n",
    "        # crops = crops.unsqueeze(1)  # Add sequence dimension (batch_size, seq_len=1, crop_size^2)\n",
    "        input = image.view(-1, 1, 28*28)\n",
    "        \n",
    "        out, hn = self.rnn(input, h0)\n",
    "\n",
    "        # Predict class and next crop center\n",
    "        class_pred = self.fc_class(out[:, -1, :])  # Class prediction\n",
    "        action_pred = self.fc_action(out[:, -1, :])  # Action (next crop center)\n",
    "\n",
    "        return class_pred, action_pred, hn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "crop_size = 20\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 10\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "num_steps = 3  # RNN steps per image\n",
    "\n",
    "# Load MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./dataset/raw_mnist', train=True, download=True, transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./dataset/raw_mnist', train=False, download=True, transform=transforms.ToTensor())\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model, optimizer, and loss functions\n",
    "model = MNISTRNN(28, hidden_size, num_layers, num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion_class = nn.CrossEntropyLoss()\n",
    "criterion_action = nn.MSELoss()  # For predicting the next center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train That Bad Boy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/938], Loss: 0.5515\n",
      "Epoch [1/10], Step [200/938], Loss: 0.7980\n",
      "Epoch [1/10], Step [300/938], Loss: 0.6671\n",
      "Epoch [1/10], Step [400/938], Loss: 0.7717\n",
      "Epoch [1/10], Step [500/938], Loss: 1.2448\n",
      "Epoch [1/10], Step [600/938], Loss: 0.9178\n",
      "Epoch [1/10], Step [700/938], Loss: 0.3193\n",
      "Epoch [1/10], Step [800/938], Loss: 1.0799\n",
      "Epoch [1/10], Step [900/938], Loss: 0.1862\n",
      "Epoch [2/10], Step [100/938], Loss: 0.4516\n",
      "Epoch [2/10], Step [200/938], Loss: 0.1952\n",
      "Epoch [2/10], Step [300/938], Loss: 0.4004\n",
      "Epoch [2/10], Step [400/938], Loss: 0.2467\n",
      "Epoch [2/10], Step [500/938], Loss: 0.3296\n",
      "Epoch [2/10], Step [600/938], Loss: 0.3747\n",
      "Epoch [2/10], Step [700/938], Loss: 0.2626\n",
      "Epoch [2/10], Step [800/938], Loss: 0.2526\n",
      "Epoch [2/10], Step [900/938], Loss: 0.0651\n",
      "Epoch [3/10], Step [100/938], Loss: 0.4308\n",
      "Epoch [3/10], Step [200/938], Loss: 0.3599\n",
      "Epoch [3/10], Step [300/938], Loss: 0.4499\n",
      "Epoch [3/10], Step [400/938], Loss: 0.1712\n",
      "Epoch [3/10], Step [500/938], Loss: 0.2011\n",
      "Epoch [3/10], Step [600/938], Loss: 0.1231\n",
      "Epoch [3/10], Step [700/938], Loss: 0.2857\n",
      "Epoch [3/10], Step [800/938], Loss: 0.2604\n",
      "Epoch [3/10], Step [900/938], Loss: 0.2133\n",
      "Epoch [4/10], Step [100/938], Loss: 0.4405\n",
      "Epoch [4/10], Step [200/938], Loss: 0.3929\n",
      "Epoch [4/10], Step [300/938], Loss: 0.2391\n",
      "Epoch [4/10], Step [400/938], Loss: 0.2835\n",
      "Epoch [4/10], Step [500/938], Loss: 0.2729\n",
      "Epoch [4/10], Step [600/938], Loss: 0.3306\n",
      "Epoch [4/10], Step [700/938], Loss: 0.0693\n",
      "Epoch [4/10], Step [800/938], Loss: 0.1936\n",
      "Epoch [4/10], Step [900/938], Loss: 0.3560\n",
      "Epoch [5/10], Step [100/938], Loss: 0.2853\n",
      "Epoch [5/10], Step [200/938], Loss: 0.0371\n",
      "Epoch [5/10], Step [300/938], Loss: 0.0175\n",
      "Epoch [5/10], Step [400/938], Loss: 0.0953\n",
      "Epoch [5/10], Step [500/938], Loss: 0.3683\n",
      "Epoch [5/10], Step [600/938], Loss: 0.0303\n",
      "Epoch [5/10], Step [700/938], Loss: 0.0750\n",
      "Epoch [5/10], Step [800/938], Loss: 0.0739\n",
      "Epoch [5/10], Step [900/938], Loss: 0.0920\n",
      "Epoch [6/10], Step [100/938], Loss: 0.2314\n",
      "Epoch [6/10], Step [200/938], Loss: 0.1722\n",
      "Epoch [6/10], Step [300/938], Loss: 0.0376\n",
      "Epoch [6/10], Step [400/938], Loss: 0.2815\n",
      "Epoch [6/10], Step [500/938], Loss: 0.0196\n",
      "Epoch [6/10], Step [600/938], Loss: 0.0099\n",
      "Epoch [6/10], Step [700/938], Loss: 0.2550\n",
      "Epoch [6/10], Step [800/938], Loss: 0.0594\n",
      "Epoch [6/10], Step [900/938], Loss: 0.3987\n",
      "Epoch [7/10], Step [100/938], Loss: 0.0170\n",
      "Epoch [7/10], Step [200/938], Loss: 0.0261\n",
      "Epoch [7/10], Step [300/938], Loss: 0.1061\n",
      "Epoch [7/10], Step [400/938], Loss: 0.0842\n",
      "Epoch [7/10], Step [500/938], Loss: 0.0436\n",
      "Epoch [7/10], Step [600/938], Loss: 0.1120\n",
      "Epoch [7/10], Step [700/938], Loss: 0.1651\n",
      "Epoch [7/10], Step [800/938], Loss: 0.0327\n",
      "Epoch [7/10], Step [900/938], Loss: 0.1636\n",
      "Epoch [8/10], Step [100/938], Loss: 0.0567\n",
      "Epoch [8/10], Step [200/938], Loss: 0.0070\n",
      "Epoch [8/10], Step [300/938], Loss: 0.0908\n",
      "Epoch [8/10], Step [400/938], Loss: 0.2242\n",
      "Epoch [8/10], Step [500/938], Loss: 0.0184\n",
      "Epoch [8/10], Step [600/938], Loss: 0.0693\n",
      "Epoch [8/10], Step [700/938], Loss: 0.1637\n",
      "Epoch [8/10], Step [800/938], Loss: 0.1577\n",
      "Epoch [8/10], Step [900/938], Loss: 0.1067\n",
      "Epoch [9/10], Step [100/938], Loss: 0.1003\n",
      "Epoch [9/10], Step [200/938], Loss: 0.1212\n",
      "Epoch [9/10], Step [300/938], Loss: 0.0114\n",
      "Epoch [9/10], Step [400/938], Loss: 0.0704\n",
      "Epoch [9/10], Step [500/938], Loss: 0.0801\n",
      "Epoch [9/10], Step [600/938], Loss: 0.0079\n",
      "Epoch [9/10], Step [700/938], Loss: 0.0279\n",
      "Epoch [9/10], Step [800/938], Loss: 0.0295\n",
      "Epoch [9/10], Step [900/938], Loss: 0.0686\n",
      "Epoch [10/10], Step [100/938], Loss: 0.2240\n",
      "Epoch [10/10], Step [200/938], Loss: 0.0209\n",
      "Epoch [10/10], Step [300/938], Loss: 0.0327\n",
      "Epoch [10/10], Step [400/938], Loss: 0.0701\n",
      "Epoch [10/10], Step [500/938], Loss: 0.1676\n",
      "Epoch [10/10], Step [600/938], Loss: 0.0157\n",
      "Epoch [10/10], Step [700/938], Loss: 0.0499\n",
      "Epoch [10/10], Step [800/938], Loss: 0.1553\n",
      "Epoch [10/10], Step [900/938], Loss: 0.0482\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        batch_size = images.size(0)\n",
    "\n",
    "        # Initialize the hidden state and center\n",
    "        h0 = torch.zeros(num_layers, batch_size, hidden_size).to(images.device)\n",
    "        centers = torch.tensor([[14, 14]] * batch_size).float().to(images.device)  # Initial center\n",
    "\n",
    "        loss = 0\n",
    "        for step in range(num_steps):\n",
    "            # Forward pass\n",
    "            class_pred, action_pred, h0 = model(images, centers, h0)\n",
    "\n",
    "            # Compute losses\n",
    "            loss_class = criterion_class(class_pred, labels)\n",
    "            # loss_action = criterion_action(action_pred, centers)  # Target is to stay at the initial center\n",
    "            loss += loss_class\n",
    "\n",
    "            # Update centers for the next step\n",
    "            # centers = torch.clip(action_pred, min=crop_size // 2, max=28 + crop_size // 2)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 97.25%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        batch_size = images.size(0)\n",
    "        h0 = torch.zeros(num_layers, batch_size, hidden_size).to(images.device)\n",
    "        \n",
    "        for step in range(num_steps):\n",
    "            class_pred, action_pred, h0 = model(images, None, h0)\n",
    "\n",
    "        _, predicted = torch.max(class_pred.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the model on the test images: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fovial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
