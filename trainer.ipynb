{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU: NVIDIA GeForce RTX 3080\n",
      "cuda:0\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available and if PyTorch is using GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available. Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "    \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch of images shape: torch.Size([64, 1, 100, 100])\n",
      "Training batch of labels shape: torch.Size([64])\n",
      "Test batch of images shape: torch.Size([64, 1, 100, 100])\n",
      "Test batch of labels shape: torch.Size([64])\n",
      "train_size: 54000, test_size: 6000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "\n",
    "class ClutteredMNISTDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Custom dataset for Cluttered MNIST.\n",
    "        :param root_dir: Root directory of the dataset (e.g., \"dataset/cluttered_mnist\")\n",
    "        :param transform: Optional torchvision transforms to apply to the images\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Gather all image paths and their labels\n",
    "        self.data = []\n",
    "        for label in range(10):  # Assuming labels are 0-9\n",
    "            label_dir = os.path.join(root_dir, str(label))\n",
    "            if os.path.isdir(label_dir):\n",
    "                for file_name in os.listdir(label_dir):\n",
    "                    if file_name.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                        file_path = os.path.join(label_dir, file_name)\n",
    "                        self.data.append((file_path, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve an image and its label at the specified index.\n",
    "        :param idx: Index of the data point\n",
    "        :return: Tuple (image, label)\n",
    "        \"\"\"\n",
    "        image_path, label = self.data[idx]\n",
    "        image = Image.open(image_path).convert('L')  # Convert to grayscale\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Define transforms for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((100, 100)),  # Resize images to 100x100\n",
    "    transforms.ToTensor(),          # Convert images to PyTorch tensors\n",
    "    # transforms.Normalize((0.1307,), (0.3081,))  # Normalize using MNIST mean and std\n",
    "])\n",
    "\n",
    "# Create dataset\n",
    "dataset_dir = \"dataset/cluttered_mnist\"\n",
    "cluttered_mnist_dataset = ClutteredMNISTDataset(root_dir=dataset_dir, transform=transform)\n",
    "\n",
    "# Split dataset into train and test (80% train, 20% test)\n",
    "train_size = int(0.9 * len(cluttered_mnist_dataset))\n",
    "test_size = len(cluttered_mnist_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(cluttered_mnist_dataset, [train_size, test_size])\n",
    "\n",
    "# DataLoader for batching and shuffling\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Example: Iterate through the DataLoader\n",
    "for images, labels in train_loader:\n",
    "    print(\"Training batch of images shape:\", images.shape)  # (batch_size, 1, 100, 100)\n",
    "    print(\"Training batch of labels shape:\", labels.shape)  # (batch_size,)\n",
    "    break\n",
    "\n",
    "for images, labels in test_loader:\n",
    "    print(\"Test batch of images shape:\", images.shape)  # (batch_size, 1, 100, 100)\n",
    "    print(\"Test batch of labels shape:\", labels.shape)  # (batch_size,)\n",
    "    break\n",
    "\n",
    "print(f\"train_size: {train_size}, test_size: {test_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from glimpse import GlimpseModel\n",
    "\n",
    "\"\"\"print('the code god was here')\"\"\"\n",
    "# Define the RNN model\n",
    "class MNISTRNN(nn.Module):\n",
    "    def __init__(self, image_size, hidden_size, num_layers, num_classes, num_kernels, device=\"cuda:0\"):\n",
    "        super(MNISTRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.image_size = image_size\n",
    "        self.num_kernels = num_kernels\n",
    "\n",
    "        # RNN to process the crops\n",
    "        self.rnn = nn.RNN(num_kernels, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc_class = nn.Linear(hidden_size, num_classes)  # Class prediction\n",
    "        self.fc_action = nn.Linear(hidden_size, 2)  # Next crop center prediction\n",
    "        \n",
    "        self.eyes = GlimpseModel((image_size, image_size), num_kernels, device)\n",
    "        \n",
    "\n",
    "    def forward(self, images, actions, h0, print_tensor=False):\n",
    "        \"\"\"\n",
    "        Forward pass with dynamic cropping and RNN processing.\n",
    "        :param image: Full input image (batch_size, 1, 28, 28)\n",
    "        :param center: Initial crop centers (batch_size, 2)\n",
    "        :param h0: Initial hidden state (num_layers, batch_size, hidden_size)\n",
    "        :return: Class prediction, next center, hidden state\n",
    "        \"\"\"\n",
    "        batch_size = len(images)\n",
    "        \n",
    "        sz = torch.ones((batch_size, 1), device=device)\n",
    "\n",
    "        \n",
    "         # TODO: This is the thing that we need to control.\n",
    "        input = images.squeeze(1)\n",
    "        output_tensor = self.eyes(input, actions, sz) # (B, 144)\n",
    "        \n",
    "        sensor_readings = output_tensor\n",
    "\n",
    "        # # Process with RNN\n",
    "        # crops = crops.unsqueeze(1)  # Add sequence dimension (batch_size, seq_len=1, crop_size^2)\n",
    "        rnn_input = output_tensor.view(batch_size, 1, self.num_kernels)\n",
    "        \n",
    "        \n",
    "        out, hn = self.rnn(rnn_input, h0)\n",
    "        \n",
    "        # Predict class and next crop center\n",
    "        class_pred = self.fc_class(out[:, -1, :])  # Class prediction\n",
    "        action_pred = torch.tanh(self.fc_action(out[:, -1, :]))  # Action (next crop center)\n",
    "\n",
    "        return class_pred, action_pred, hn, sensor_readings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "image_size = 100\n",
    "hidden_size = 512\n",
    "num_layers = 2\n",
    "num_classes = 10\n",
    "batch_size = 64\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 10\n",
    "num_steps = 3  # RNN steps per image\n",
    "num_kernels = 12*12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model, optimizer, and loss functions\n",
    "model = MNISTRNN(image_size, hidden_size, num_layers, num_classes, num_kernels, device)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion_class = nn.CrossEntropyLoss()\n",
    "criterion_action = nn.MSELoss()  # For predicting the next center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train That Bad Boy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/844], Loss: 6.9069\n",
      "Epoch [1/10], Step [200/844], Loss: 6.9585\n",
      "Epoch [1/10], Step [300/844], Loss: 6.8328\n",
      "Epoch [1/10], Step [400/844], Loss: 6.7515\n",
      "Epoch [1/10], Step [500/844], Loss: 6.9301\n",
      "Epoch [1/10], Step [600/844], Loss: 6.7001\n",
      "Epoch [1/10], Step [700/844], Loss: 6.7893\n",
      "Epoch [1/10], Step [800/844], Loss: 6.6876\n",
      "Epoch [2/10], Step [100/844], Loss: 6.7203\n",
      "Epoch [2/10], Step [200/844], Loss: 6.6029\n",
      "Epoch [2/10], Step [300/844], Loss: 6.6873\n",
      "Epoch [2/10], Step [400/844], Loss: 6.6738\n",
      "Epoch [2/10], Step [500/844], Loss: 6.4596\n",
      "Epoch [2/10], Step [600/844], Loss: 6.7748\n",
      "Epoch [2/10], Step [700/844], Loss: 6.5888\n",
      "Epoch [2/10], Step [800/844], Loss: 6.5548\n",
      "Epoch [3/10], Step [100/844], Loss: 6.5364\n",
      "Epoch [3/10], Step [200/844], Loss: 6.8037\n",
      "Epoch [3/10], Step [300/844], Loss: 6.4117\n",
      "Epoch [3/10], Step [400/844], Loss: 6.3715\n",
      "Epoch [3/10], Step [500/844], Loss: 6.3504\n",
      "Epoch [3/10], Step [600/844], Loss: 6.6713\n",
      "Epoch [3/10], Step [700/844], Loss: 6.5381\n",
      "Epoch [3/10], Step [800/844], Loss: 6.5057\n",
      "Epoch [4/10], Step [100/844], Loss: 6.3744\n",
      "Epoch [4/10], Step [200/844], Loss: 6.4028\n",
      "Epoch [4/10], Step [300/844], Loss: 6.4163\n",
      "Epoch [4/10], Step [400/844], Loss: 6.3488\n",
      "Epoch [4/10], Step [500/844], Loss: 6.2601\n",
      "Epoch [4/10], Step [600/844], Loss: 6.3363\n",
      "Epoch [4/10], Step [700/844], Loss: 6.3094\n",
      "Epoch [4/10], Step [800/844], Loss: 6.6394\n",
      "Epoch [5/10], Step [100/844], Loss: 6.2084\n",
      "Epoch [5/10], Step [200/844], Loss: 6.1422\n",
      "Epoch [5/10], Step [300/844], Loss: 6.3499\n",
      "Epoch [5/10], Step [400/844], Loss: 6.3748\n",
      "Epoch [5/10], Step [500/844], Loss: 6.5172\n",
      "Epoch [5/10], Step [600/844], Loss: 6.4127\n",
      "Epoch [5/10], Step [700/844], Loss: 6.3974\n",
      "Epoch [5/10], Step [800/844], Loss: 6.4696\n",
      "Epoch [6/10], Step [100/844], Loss: 6.2812\n",
      "Epoch [6/10], Step [200/844], Loss: 6.4899\n",
      "Epoch [6/10], Step [300/844], Loss: 6.1327\n",
      "Epoch [6/10], Step [400/844], Loss: 6.4108\n",
      "Epoch [6/10], Step [500/844], Loss: 6.3215\n",
      "Epoch [6/10], Step [600/844], Loss: 6.0867\n",
      "Epoch [6/10], Step [700/844], Loss: 6.2009\n",
      "Epoch [6/10], Step [800/844], Loss: 6.3928\n",
      "Epoch [7/10], Step [100/844], Loss: 6.2399\n",
      "Epoch [7/10], Step [200/844], Loss: 6.4573\n",
      "Epoch [7/10], Step [300/844], Loss: 6.5251\n",
      "Epoch [7/10], Step [400/844], Loss: 5.9580\n",
      "Epoch [7/10], Step [500/844], Loss: 5.8708\n",
      "Epoch [7/10], Step [600/844], Loss: 6.2555\n",
      "Epoch [7/10], Step [700/844], Loss: 6.5146\n",
      "Epoch [7/10], Step [800/844], Loss: 6.2340\n",
      "Epoch [8/10], Step [100/844], Loss: 6.3700\n",
      "Epoch [8/10], Step [200/844], Loss: 6.1508\n",
      "Epoch [8/10], Step [300/844], Loss: 6.4228\n",
      "Epoch [8/10], Step [400/844], Loss: 6.3131\n",
      "Epoch [8/10], Step [500/844], Loss: 6.2750\n",
      "Epoch [8/10], Step [600/844], Loss: 6.3178\n",
      "Epoch [8/10], Step [700/844], Loss: 6.3632\n",
      "Epoch [8/10], Step [800/844], Loss: 6.2836\n",
      "Epoch [9/10], Step [100/844], Loss: 6.7289\n",
      "Epoch [9/10], Step [200/844], Loss: 6.0910\n",
      "Epoch [9/10], Step [300/844], Loss: 6.4111\n",
      "Epoch [9/10], Step [400/844], Loss: 6.4572\n",
      "Epoch [9/10], Step [500/844], Loss: 6.5955\n",
      "Epoch [9/10], Step [600/844], Loss: 6.1034\n",
      "Epoch [9/10], Step [700/844], Loss: 6.4704\n",
      "Epoch [9/10], Step [800/844], Loss: 6.2510\n",
      "Epoch [10/10], Step [100/844], Loss: 6.1135\n",
      "Epoch [10/10], Step [200/844], Loss: 6.2871\n",
      "Epoch [10/10], Step [300/844], Loss: 6.2915\n",
      "Epoch [10/10], Step [400/844], Loss: 6.2427\n",
      "Epoch [10/10], Step [500/844], Loss: 6.4129\n",
      "Epoch [10/10], Step [600/844], Loss: 6.4227\n",
      "Epoch [10/10], Step [700/844], Loss: 6.5755\n",
      "Epoch [10/10], Step [800/844], Loss: 6.3340\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        batch_size = images.size(0)\n",
    "\n",
    "        # Initialize the hidden state and center\n",
    "        h0 = torch.zeros(num_layers, batch_size, hidden_size).to(images.device)\n",
    "        \n",
    "        next_actions = torch.zeros((batch_size, 2), device=images.device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = 0\n",
    "        for step in range(num_steps):\n",
    "            # Print the image out\n",
    "            \n",
    "            # next_actions = torch.zeros_like(next_actions, device=next_actions.device) #TODO: remove this line please\n",
    "            # Forward pass    \n",
    "            class_pred, action_pred, h0, sensor_readings = model(images, next_actions, h0)\n",
    "            \n",
    "            if (i + 1) % 100 == 0:\n",
    "                sz = torch.ones((1), device=device)\n",
    "                img = images[0][0]\n",
    "                sc = next_actions[0]\n",
    "                img_name = f\"pictures/iter{i+1}_step{step}.png\"\n",
    "                model.eyes.plot_image(img_name, img, sc, sz, sensor_readings[0])\n",
    "            \n",
    "            # Compute losses\n",
    "            loss_class = criterion_class(class_pred, labels)\n",
    "            loss += loss_class\n",
    "            \n",
    "            \n",
    "            next_actions = action_pred\n",
    "            \n",
    "            # print(f\"step{step}: memory update\")\n",
    "            # print(\"\", torch.cuda.memory_summary(device='cuda:0'))\n",
    "\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "radius =  -5.1536255\n",
      "radius =  3.8306072\n",
      "radius =  26.069347\n",
      "radius =  17.39091\n",
      "radius =  23.741621\n",
      "radius =  -2.0318391\n",
      "radius =  -7.843869\n",
      "radius =  5.827407\n",
      "radius =  6.8062706\n",
      "radius =  4.826289\n",
      "radius =  12.667022\n",
      "radius =  9.263925\n",
      "radius =  4.3573437\n",
      "radius =  0.7361411\n",
      "radius =  11.355462\n",
      "radius =  29.792902\n",
      "radius =  14.283136\n",
      "radius =  -4.74332\n",
      "radius =  -4.6452847\n",
      "radius =  5.826675\n",
      "radius =  6.794176\n",
      "radius =  4.8588367\n",
      "radius =  5.94159\n",
      "radius =  0.75635684\n",
      "radius =  15.912837\n",
      "radius =  12.480851\n",
      "radius =  13.584882\n",
      "radius =  13.948655\n",
      "radius =  8.889308\n",
      "radius =  -3.6260238\n",
      "radius =  3.2540605\n",
      "radius =  4.66995\n",
      "radius =  0.50158477\n",
      "radius =  10.785669\n",
      "radius =  3.3065057\n",
      "radius =  -5.704312\n",
      "radius =  10.272447\n",
      "radius =  4.512081\n",
      "radius =  1.3129271\n",
      "radius =  10.007586\n",
      "radius =  0.9228607\n",
      "radius =  8.837471\n",
      "radius =  13.132521\n",
      "radius =  9.378915\n",
      "radius =  14.5801525\n",
      "radius =  11.872587\n",
      "radius =  12.6866865\n",
      "radius =  19.421028\n",
      "radius =  12.890298\n",
      "radius =  9.131781\n",
      "radius =  -3.5445204\n",
      "radius =  -2.5432472\n",
      "radius =  -1.1058468\n",
      "radius =  7.317191\n",
      "radius =  15.166267\n",
      "radius =  7.9905033\n",
      "radius =  7.4760876\n",
      "radius =  14.54763\n",
      "radius =  12.422736\n",
      "radius =  9.692965\n",
      "radius =  7.0427866\n",
      "radius =  -3.5882823\n",
      "radius =  7.753508\n",
      "radius =  -4.8016915\n",
      "radius =  4.9073687\n",
      "radius =  15.059129\n",
      "radius =  -0.9100444\n",
      "radius =  8.747835\n",
      "radius =  7.315875\n",
      "radius =  10.352993\n",
      "radius =  -7.530706\n",
      "radius =  5.4445977\n",
      "radius =  -3.0793061\n",
      "radius =  12.765187\n",
      "radius =  9.904092\n",
      "radius =  4.56709\n",
      "radius =  -11.432648\n",
      "radius =  3.1421266\n",
      "radius =  18.026964\n",
      "radius =  -5.7185717\n",
      "radius =  12.9014225\n",
      "radius =  10.360377\n",
      "radius =  18.012037\n",
      "radius =  -10.511264\n",
      "radius =  -4.979876\n",
      "radius =  3.088783\n",
      "radius =  12.181924\n",
      "radius =  -6.3284626\n",
      "radius =  5.984567\n",
      "radius =  -5.1892366\n",
      "radius =  16.203142\n",
      "radius =  8.993626\n",
      "radius =  0.9820885\n",
      "radius =  1.0073835\n",
      "radius =  -0.74004084\n",
      "radius =  -0.9745747\n",
      "radius =  4.4021106\n",
      "radius =  10.750337\n",
      "radius =  -2.4532647\n",
      "radius =  3.1148732\n",
      "radius =  -7.8754992\n",
      "radius =  10.937206\n",
      "radius =  5.067877\n",
      "radius =  10.503328\n",
      "radius =  -12.682608\n",
      "radius =  4.293438\n",
      "radius =  -7.4499254\n",
      "radius =  -7.185912\n",
      "radius =  4.2799597\n",
      "radius =  6.7903476\n",
      "radius =  13.577098\n",
      "radius =  1.4217119\n",
      "radius =  -5.566709\n",
      "radius =  13.8843155\n",
      "radius =  5.140956\n",
      "radius =  -1.1731282\n",
      "radius =  -8.782905\n",
      "radius =  -4.1777015\n",
      "radius =  12.3511715\n",
      "radius =  10.53525\n",
      "radius =  -2.8667967\n",
      "radius =  11.332243\n",
      "radius =  7.945752\n",
      "radius =  0.712761\n",
      "radius =  1.5292706\n",
      "radius =  5.6097536\n",
      "radius =  -0.8423088\n",
      "radius =  12.925655\n",
      "radius =  3.1202202\n",
      "radius =  1.0036647\n",
      "radius =  -0.8487173\n",
      "radius =  6.535396\n",
      "radius =  10.188653\n",
      "radius =  -1.9465636\n",
      "radius =  3.104632\n",
      "radius =  -0.7588499\n",
      "radius =  4.2938433\n",
      "radius =  -1.6935625\n",
      "radius =  8.366452\n",
      "radius =  11.759343\n",
      "radius =  21.431612\n",
      "radius =  10.425083\n",
      "radius =  11.736212\n",
      "radius =  10.318593\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "# print(model.eyes.mu)\n",
    "# print(model.eyes.sigma)\n",
    "\n",
    "background_img = torch.zeros((100, 100))\n",
    "sc = torch.zeros((2), device=device)\n",
    "sz = torch.ones((1), device=device)\n",
    "\n",
    "# Plot an image\n",
    "image_name = \"out.png\"\n",
    "model.eyes.plot_image(image_name, background_img, sc, sz)\n",
    "img = mpimg.imread(image_name)\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")  # Turn off axis labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        batch_size = images.size(0)\n",
    "        h0 = torch.zeros(num_layers, batch_size, hidden_size).to(images.device)\n",
    "        \n",
    "        for step in range(num_steps):\n",
    "            class_pred, action_pred, h0 = model(images, None, h0)\n",
    "\n",
    "        _, predicted = torch.max(class_pred.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the model on the test images: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mani",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
