{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch of images shape: torch.Size([64, 1, 100, 100])\n",
      "Training batch of labels shape: torch.Size([64])\n",
      "Test batch of images shape: torch.Size([64, 1, 100, 100])\n",
      "Test batch of labels shape: torch.Size([64])\n",
      "train_size: 54000, test_size: 6000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "57600"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "\n",
    "class ClutteredMNISTDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Custom dataset for Cluttered MNIST.\n",
    "        :param root_dir: Root directory of the dataset (e.g., \"dataset/cluttered_mnist\")\n",
    "        :param transform: Optional torchvision transforms to apply to the images\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Gather all image paths and their labels\n",
    "        self.data = []\n",
    "        for label in range(10):  # Assuming labels are 0-9\n",
    "            label_dir = os.path.join(root_dir, str(label))\n",
    "            if os.path.isdir(label_dir):\n",
    "                for file_name in os.listdir(label_dir):\n",
    "                    if file_name.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                        file_path = os.path.join(label_dir, file_name)\n",
    "                        self.data.append((file_path, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve an image and its label at the specified index.\n",
    "        :param idx: Index of the data point\n",
    "        :return: Tuple (image, label)\n",
    "        \"\"\"\n",
    "        image_path, label = self.data[idx]\n",
    "        image = Image.open(image_path).convert('L')  # Convert to grayscale\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Define transforms for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((100, 100)),  # Resize images to 100x100\n",
    "    transforms.ToTensor(),          # Convert images to PyTorch tensors\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # Normalize using MNIST mean and std\n",
    "])\n",
    "\n",
    "# Create dataset\n",
    "dataset_dir = \"dataset/cluttered_mnist\"\n",
    "cluttered_mnist_dataset = ClutteredMNISTDataset(root_dir=dataset_dir, transform=transform)\n",
    "\n",
    "# Split dataset into train and test (80% train, 20% test)\n",
    "train_size = int(0.9 * len(cluttered_mnist_dataset))\n",
    "test_size = len(cluttered_mnist_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(cluttered_mnist_dataset, [train_size, test_size])\n",
    "\n",
    "# DataLoader for batching and shuffling\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Example: Iterate through the DataLoader\n",
    "for images, labels in train_loader:\n",
    "    print(\"Training batch of images shape:\", images.shape)  # (batch_size, 1, 100, 100)\n",
    "    print(\"Training batch of labels shape:\", labels.shape)  # (batch_size,)\n",
    "    break\n",
    "\n",
    "for images, labels in test_loader:\n",
    "    print(\"Test batch of images shape:\", images.shape)  # (batch_size, 1, 100, 100)\n",
    "    print(\"Test batch of labels shape:\", labels.shape)  # (batch_size,)\n",
    "    break\n",
    "\n",
    "print(f\"train_size: {train_size}, test_size: {test_size}\")\n",
    "64*900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from glimpse import GlimpseModel\n",
    "\n",
    "\"\"\"print('the code god was here')\"\"\"\n",
    "# Define the RNN model\n",
    "class MNISTRNN(nn.Module):\n",
    "    def __init__(self, image_size, hidden_size, num_layers, num_classes, num_kernels):\n",
    "        super(MNISTRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.image_size = image_size\n",
    "\n",
    "        # RNN to process the crops\n",
    "        self.rnn = nn.RNN(num_kernels, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc_class = nn.Linear(hidden_size, num_classes)  # Class prediction\n",
    "        self.fc_action = nn.Linear(hidden_size, 2)  # Next crop center prediction\n",
    "        \n",
    "        self.eyes = GlimpseModel((image_size, image_size), num_kernels)\n",
    "        \n",
    "        self.sc = torch.zeros((batch_size, 2))\n",
    "        self.sz = torch.ones((batch_size, 1))\n",
    "\n",
    "    def crop(self, padded_image, center):\n",
    "        \"\"\"\n",
    "        Crop a region around the given center from the padded image.\n",
    "        :param padded_image: Padded input image (batch_size, 1, padded_size, padded_size)\n",
    "        :param center: Crop centers (batch_size, 2)\n",
    "        :return: Cropped image regions (batch_size, crop_size, crop_size)\n",
    "        \"\"\"\n",
    "        crop_size = self.crop_size\n",
    "        half_crop = crop_size // 2\n",
    "\n",
    "        # Compute cropping indices\n",
    "        x_start = (center[:, 0] - half_crop).long()\n",
    "        x_end = (center[:, 0] + half_crop).long()\n",
    "        y_start = (center[:, 1] - half_crop).long()\n",
    "        y_end = (center[:, 1] + half_crop).long()\n",
    "\n",
    "        # Perform efficient tensor slicing\n",
    "        crops = torch.stack([\n",
    "            padded_image[b, :, y_start[b].item():y_end[b].item(), x_start[b].item():x_end[b].item()]\n",
    "            for b in range(padded_image.size(0))\n",
    "        ])\n",
    "        return crops\n",
    "\n",
    "    def forward(self, images, center, h0):\n",
    "        \"\"\"\n",
    "        Forward pass with dynamic cropping and RNN processing.\n",
    "        :param image: Full input image (batch_size, 1, 28, 28)\n",
    "        :param center: Initial crop centers (batch_size, 2)\n",
    "        :param h0: Initial hidden state (num_layers, batch_size, hidden_size)\n",
    "        :return: Class prediction, next center, hidden state\n",
    "        \"\"\"\n",
    "        batch_size = len(images)\n",
    "        \n",
    "         # TODO: This is the thing that we need to control.\n",
    "        input = images.squeeze(1)\n",
    "        output_tensor = self.eyes(input, self.sc, self.sz) # (B, 144)\n",
    "\n",
    "        # # Process with RNN\n",
    "        # crops = crops.unsqueeze(1)  # Add sequence dimension (batch_size, seq_len=1, crop_size^2)\n",
    "        rnn_input = output_tensor.view(batch_size, 1, 144)\n",
    "        \n",
    "        \n",
    "        out, hn = self.rnn(rnn_input, h0)\n",
    "\n",
    "        # Predict class and next crop center\n",
    "        class_pred = self.fc_class(out[:, -1, :])  # Class prediction\n",
    "        action_pred = self.fc_action(out[:, -1, :])  # Action (next crop center)\n",
    "\n",
    "        return class_pred, action_pred, hn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "image_size = 100\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 10\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "num_steps = 3  # RNN steps per image\n",
    "num_kernels = 12*12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model, optimizer, and loss functions\n",
    "model = MNISTRNN(image_size, hidden_size, num_layers, num_classes, num_kernels)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion_class = nn.CrossEntropyLoss()\n",
    "criterion_action = nn.MSELoss()  # For predicting the next center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train That Bad Boy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "5\n",
      "6\n",
      "Completed a step\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Completed a step\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Completed a step\n",
      "Epoch [1/10], Step [1/844], Loss: 6.9147\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Completed a step\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Completed a step\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Completed a step\n",
      "Epoch [1/10], Step [2/844], Loss: 7.0246\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Completed a step\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Completed a step\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Completed a step\n",
      "Epoch [1/10], Step [3/844], Loss: 7.0200\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Completed a step\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Completed a step\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Completed a step\n",
      "Epoch [1/10], Step [4/844], Loss: 6.9341\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Completed a step\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Completed a step\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Completed a step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 27\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# Update centers for the next step\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# centers = torch.clip(action_pred, min=crop_size // 2, max=28 + crop_size // 2)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 27\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/fovial/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/fovial/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        batch_size = images.size(0)\n",
    "\n",
    "        # Initialize the hidden state and center\n",
    "        h0 = torch.zeros(num_layers, batch_size, hidden_size).to(images.device)\n",
    "        centers = torch.tensor([[14, 14]] * batch_size).float().to(images.device)  # Initial center\n",
    "\n",
    "        loss = 0\n",
    "        for step in range(num_steps):\n",
    "            # Forward pass\n",
    "            class_pred, action_pred, h0 = model(images, centers, h0)\n",
    "\n",
    "            # Compute losses\n",
    "            loss_class = criterion_class(class_pred, labels)\n",
    "            # loss_action = criterion_action(action_pred, centers)  # Target is to stay at the initial center\n",
    "            loss += loss_class\n",
    "            \n",
    "            print(\"Completed a step\")\n",
    "\n",
    "            # Update centers for the next step\n",
    "            # centers = torch.clip(action_pred, min=crop_size // 2, max=28 + crop_size // 2)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 1 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 97.25%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        batch_size = images.size(0)\n",
    "        h0 = torch.zeros(num_layers, batch_size, hidden_size).to(images.device)\n",
    "        \n",
    "        for step in range(num_steps):\n",
    "            class_pred, action_pred, h0 = model(images, None, h0)\n",
    "\n",
    "        _, predicted = torch.max(class_pred.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the model on the test images: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fovial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
